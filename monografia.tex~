%% LyX 1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[brazil,ruledheader, a4paper]{abnt}
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage[brazil]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm,amsfonts}
\usepackage{graphicx,color}
\usepackage{amsthm}
\usepackage[noend, ruled, linesnumbered]{algorithm2e}
\usepackage{subfig}
\usepackage{subfloat}

%Nomes dos membros da banca
\newcommand{\bancaPrimeiroNome}{Nome do membro da banca}
\newcommand{\bancaPrimeiroInstituicao}{Intituição do Membro}
\newcommand{\bancaSegundoNome}{Nome do membro da banca}
\newcommand{\bancaSegundoInstituicao}{Intituição do Membro}

\input{common}

\makeatletter
\usepackage{babel}
\makeatother
\begin{document}

%Nome do aluno
\autor{Luís Henrique Carvalho do Nascimento}

%titulo do trabalho
\titulo{Avaliação de Parâmetros da Otimização por Enxame de Partículas}

%orientado
\orientador{Eduardo Krempser da Silva}

%coorientador 
\coorientador{Hélio José Corrêa Barbosa}

\comentario{Trabalho apresentado no curso de Formação em 
Tecnologia da Informação e Comunicação da FAETERJ -- Petrópolis 
como requisito parcial para obtenção do grau de tecnólogo.}


\instituicao{Faculdade de Educação Tecnológica do Estado do Rio de Janeiro}


\local{Petrópolis -- RJ}


\data{setembro de 2013}

%Criando a capa com estilo diferente do \capa no ABNTEX
\begin{titlepage}
    \begin{center}
		\centering
		\includegraphics[width=2.0cm]{logo.pdf}\\
        {\textbf{GOVERNO DO ESTADO DO RIO DE JANEIRO\\
	        SECRETARIA DE ESTADO DE CIÊNCIA E TECNOLOGIA\\
		FUNDAÇÃO DE APOIO À ESCOLA TÉCNICA\\
		CENTRO DE EDUCAÇÃO TECNOLÓGICA DO ESTADO DO RIO DE JANEIRO\\
		FACULDADE DE EDUCAÇÃO TECNOLÓGICA DO ESTADO DO RIO DE JANEIRO\\
		FAETERJ/PETRÓPOLIS}
	} \\ [4.9cm]
        {\Huge \ABNTtitulodata} \\[4.9cm]
        \vfill
        {\large \textbf{\ABNTautordata}} \\[4.9cm]
        {\large PETRÓPOLIS} \\
        {\large \ABNTdatadata}
    \end{center}
\end{titlepage}
%fim da capa

% Folha de rosto sem o uso de \folhaderosto
\begin{titlepage}
    \begin{center}
%        {\large \ABNTtitulodata} \\
%        {\large \ABNTautordata} \\[3.3cm]
        {\Huge \ABNTtitulodata} \\[3.3cm]
        {\large \ABNTautordata} \\[3.3cm]
        \hspace{.45\textwidth} % posicionando a minipage
        \begin{minipage}{.5\textwidth}
            \begin{espacosimples}
                Trabalho apresentado no curso de Formação em 
		Tecnologia da Informação e Comunicação da FAETERJ -- 
		Petrópolis como requisito parcial para 
		obtenção do grau de tecnólogo.
            \end{espacosimples}
        \end{minipage}
\end{center}
Orientador: \ABNTorientadordata \\ Co-orientador: Hélio José Corrêa Barbosa
\begin{center}
        \vfill
        {\large PETRÓPOLIS} \\
        {\large \ABNTdatadata}
    \end{center}
\end{titlepage}


\begin{folhadeaprovacao}
Monografia de Projeto Final de Graduação sob o título \textit{``\ABNTtitulodata''},
defendida por \ABNTautordata~e aprovada em \ABNTdatadata, em Petrópolis,
Estado do Rio de Janeiro, pela banca examinadora constituída pelos
professores: \setlength{\ABNTsignthickness}{0.4pt}

\assinatura{\orientador\\ Orientador} \assinatura{\coorientador\\ Co-orientador} \assinatura{\bancaPrimeiroNome\\ \bancaPrimeiroInstituicao} \assinatura{\bancaSegundoNome\\ \bancaPrimeiroInstituicao}
\end{folhadeaprovacao}


\begin{resumo}
	Atualmente o uso de meta-heur\'isticas
	vem sendo utilizada por sua simplicidade de implemente\c c\~ao e por abranger uma vasta 
	gama de problemas com os mais diversos n\'iveis de complexidade. 
	Neste trabalho estudamos a t\'ecnica de otimiza\c c\~ao denominada Otimiza\c c\~ao por 
	Enxame de Part\'iculas, Particle Swarm Optimization (PSO). 
	A qual, assim como ocorre em grande parte das meta-heur\'isticas, a qualidade 
	de aplica\c c\~ao dessa t\'ecnica deve definir alguns par\^ametros que influ\^enciam 
	diretamente no desempenho do algoritmo, como a in\'ercia que impede que uma 
	part\'icula mude de dire\c c\~ao instantaneamente, e a capacidade de aprendizado 
	coletivo e individual da part\'icula. Esses par\^ametros foram avaliados por um 
	conjunto de testes amplamente utilizado na literatura e com o objetivo de 
	observar como o algoritmo se comporta com cada grupo de par\^ametro em diferentes 
	situa\c c\^oes a ele apresentado. 
	
	Al\'em disso, um estudo mais amplo a respeito da t\'ecnica foi feito avaliando 
	algoritmos que adaptam seus par\^ametros no decorrer da execu\c c\~ao, buscando 
	assim uma ferramenta mais eficaz aplicada em diferentes classes de problemas. 
	Algoritmos que adaptam a in\'ercia por expemplo, geralmente permitem que a 
	part\'icula permanessa mais livre no in\'icio da execu\c c\~ao do programa e tornam-se 
	mais restritivas com o decorrer da busca. 
	
	As part\'iculas por sua vez, n\~ao dependem apenas da in\'ercia para evoluirem. Seu
	aprendizado se da por dois par\^ametros conhecidos como acelera\c c\~ao cognitiva e 
	aclera\c c\~ao social, que dizem o quanto uma part\'icula \'e influenciada para seguir o 
	aprendizado do enxame e o quanto ela deve buscar por si pr\'opria o ponto \'otimo.
	(Sendo assim, se uma execu\c c\~ao mais social, ou seja, que d\^e mais importancia para
	o aprendizado do enxame, pode levar ao fracasso de todo o grupo caso ele se perca.)
	Com isso, fica claro que a defini\c c\~ao desses par\^ametros compromete o resultado final da t\'ecnica.
	
	Al\'em desses tipos de adapta\c c\~ao, existem algoritimos que atualizam a 
	velocidade das part\'iculas de maneiras diferentes de acordo com o que as 
	part\'iculas aprenderam, bem como a inclus\~ao de novos par\^ametros necess\'arios 
	ao controle da adapt\c c\~ao.
	
% 	Um trabalho ainda mais elaborado foi o uso de uma técnica de otimização com algoritmos
% 	híbridos paralelizados. O uso do hibridismo 
	
	Portanto, este trabalho visa avaliar diversas propostas em torno da defini\c c\~ao 
	dos par\^ametros do PSO em um conjunto de problemas de otimiza\c c\~ao sem restri\c c\~oes, 
	visando identificar as propostas mais adequadas para diferentes classes de problemas.
	
	Este trabalho, além de avaliar os parâmetros do PSO, ainda faz o uso do PSO como mais 
	uma metaheurística em uma técnica de otimização hibrida, que procura aperfeiçoar
	o processo de otimização fazendo o uso de mais de uma técnica. Pelo fato da busca ser demorada
	utilizamos uma técnica de paralelização para diminuir o tempo de processo.
\end{resumo}
\include{abstract}

\include{dedicatoria}


\chapter*{Agradecimentos}

Agradeço a ...

\tableofcontents{}\listoffigures



\listoftables


\chapter{Introdução\label{cap:introducao}}

% Introdução

	Atualmente, 

\chapter{Otimização\label{cap:otimizacao}}
	  Este caítulo descreve um pouco sobre otimização, o qual é o centro da base
	  do trabalho desenvolvido.
	  
	  \section{Otimização\label{sec:otimizacao}}
		  Otimização é o processo de tornar algo melhor. Consiste em tentar variações
		  de um conceito inicial e usar informações obtidas pra melhorar esta ideia.
		  Quando otmizamos algo, algumas questões podem surgir. A solução encontrada
		  é a única solução? Na maioria das vezes não. É a melhor solução? Essa, é
		  uma pergunta díficil de ser respondida. Logo, otimização e uma ferramenta da
		  matemática que nós recorremos para termos essa resposta.
		  
		  Otimização, mesmo que instuitivamente, é buscado por todos no seu cotidiano.
		  Um exemplo disso, é a rota de sua casa até o seu trabalho. Nesse aspecto, você
		  pode optar por otimizar tempo. Como passa todo dia pelo trajeto, sabe-se onde
		  tem mais trânsito. Quando se evita o caminho por onde pega mais trânsito, 
		  consequentemente, reduz o tempo de casa até o trabalho.
		  Ou ainda pode optar por onde tenha a menor distância, ainda que
		  leve um tempo maior. Esse tipo de busca  é
		  conhecida como busca exaustiva. 
		  
		  A otimização faz parte de um ramo interdisciplinar da matemática aplicada, fazendo
		  uso de modelos matemáticos, estátisticos e de algoritmos na ajuda da tomada de decisão.
		  O problema em questão, pode ser de minimização ou maximização de uma dada função, a
		  qual pode ser deniminada \textbf{função alvo} ou \textbf{função objetivo}, 
		  além disso, um problema de otimização ainda existe um conjunto de \textbf{restrições}, 
		  (porém problemas desse tipo não serão analisados neste trabalho)
		  ambos relacionados às \textbf{variáveis de decisão}.
		  
		  Informalmente, otimização pode ser descrita por: 
		  \begin{equation}
			Minimizar \quad f(x) \quad  sujeita \quad  a \quad  x \in \Omega \subset \mathbb{R}^n
		  \end{equation}\label{eq:otimizacao}
		  
		  Onde a função $f$ é a \textit{função objetivo}, e o conjunto $\Omega$ é o 
		  \textit{conjunto factível}.
		  


\chapter{Metaheurística\label{cap:meta-heuristica}}
	Neste capítulo será descrito brevemente sobre o que vem a ser uma 
	heurística e meta-herística e ainda comentar sobre  problemas de se utilizar uma
	mera-heuristica para problemas de otimização e as principais vantagens
	dessa utilização.
	\section{Heurística e Meta-Heurística\label{sec:meta-heuristica}}
		Primeiramente, devemos entender o que vem a ser uma heurística. Nada
		mais é do que uma estratégia de como resolver um determinado tipo de
		problema. A maneira como você irá resolver o problema em questão é
		conhecida como heurística.
		
		Já a Metaheurística, nada mais é do que uma Heurística de forma mais
		ampla, ou seja, ela é uma heurística ``genérica''. Enquanto a Heurística
		é a maneira como você soluciona um problema, a Metaheurística, soluciona
		vários problemas.
		
	\section{Problemas e Vantagens da Meta-Heurística}
	
		Como já dito anteriormente, ela é aplicavel a uma vasta gama de problemas, e
		essa é a principal vantagem de se utilizar uma metaheurística. Porém 
		essa vantagem abre espaço para um ponto fraco. Por não ser algo específico pra 
		um determinado problema, é extremamente normal que ela não obtenha a melhor solução no final,
		claro que ela pode encontrar o ponto ótimo, mas ela não garante que seja.
		Contudo, os problemas de otimização, no geral, não exigem a melhor solução
		possível. Mesmo não sendo uma exigencia, é claro, que buscamos aprimorar a ferramenta
		de busca para uma melhor solução final.
	  
\chapter{Otimização Por Enxame de Particulas\label{cap:PSO}}
	Neste capítulo, será introduzido a técnica de otimização e mostraremos a influência sofrida 
	no algoritmo sobre os parâmetros que devem ser definidos para que a aplicação tenha um bom desempenho.
	Além disso, será descrito algumas adaptações da técnica inicial que tentam deixar a aplicação
	mais independente dos parâmetros.
\section{\textit{Particle Swarm Optimization}\label{sec:PSO}}

	Criado por Kennedy e Eberhart, a técnica de otimização conhecida como Otimização
	por Enxame de Partículas\cite{kennedy1995particle}, do ingles \textit{Particle Swarm Optimization} (PSO),
	faz parte de uma grande área da computação, denominada computação evolucionista, 
	onde o aprendizado do algoritmo ocorre no meio do processo evolutivo.
	O PSO é inspirado no comportamento social de um bando de pássaros na busca por alimentos.
	Nessa busca, os pássaros aprendem não somente com suas próprias experiências, mas também com a experência
	obtida pelo bando. 
	A técnica faz uma analogia ao bando, onde o líder do bando guia os demais pássaros, porém
	esse líder pode ser substituido por um outro pássaro do bando. O que vai dizer quando cada
	pássaro é o líder, é o que ele aprendeu. Assim, ele é conhecido como Gbest, ou seja,
	é a partícula que obteve melhor aptidão.
	
	\begin{equation}
		V_{id} = V_{id} + C_1 * rand() * (P_{id} - X_{id}) + 
		C_2 * rand() * (P_{gd} - X_{id})
		\label{eq:PSO}
	\end{equation}
	\begin{equation}
		X_{id} = X_{id} + V_{id}
		\label{eq:Local}
	\end{equation}
	onde, 
	\begin{itemize}
	 \item $V_{id}$ \'{e} a velocidade da part\'{i}cula em cada dimensão, 
	 \item $P_{id}$ \'{e} a posição onde foi encontrado o Pbest, 
	 \item $P_{gd}$ \'{e} a posição onde foi encontrado o Gbest, 
	 \item $X_{id}$ \'{e} a posição atual da part\'{i}cula, 
	 \item $C_1$ e $C_2$ são as componentes cognitivas e sociais e 
	 \item $rand()$ uma func\~{a}o aleat\'oria no intervalo [0, 1].
	\end{itemize}


\section{\textit{Global Best Particle Swarm Optimization}\label{sec:GBPSO}}
	O GBPSO é uma variante do PSO onde adiciona o peso da inércia a velocidade da partícula
	a cada iteração. Isso foi feito pois a partícula tem uma tendencia natural de se perder
	no meio do processo evolutivo, o que é chamado de ``\textit{explosão}''. O peso da inércia, 
	representado por $W$, é uma constante que geralmente está no intervalo de [0.4; 0.9], 
	porém vários outros valores foram encontrados na literatura.
	
	A inércia, é a tendencia natural de um corpo que está em movimento tem de continuar em
	movimento. O que faz esse corpo parar, são outras forças que agem sobre ele.
	Já um corpo que se encontra em repouso, tem a tendencia de continuar em repouso.
	Esse fenômeno, foi adicionado ao algorítmo para conter a explosão do enxame.
	A atualização da velocidade das part\'iculas \'e feita segundo a Eq. \ref{eq:GBPSO}
	
	\begin{equation}
		V_{id} = w * V_{id} + C_1 * rand() * (P_{id} - X_{id}) + 
		C_2 * rand() * (P_{gd} - X_{id})
		\label{eq:GBPSO}
	\end{equation}
\section{\textit{Decreasing Weight Particle Swarm Optimization}\label{sec:DWPSO}}
	Uma área muito estudada atualmente adapta o algoritmo no decorrer da aplicação,
	e com bastante campo ainda em aberto (como mostraremos no capítulo \ref{cap:ExpComp}
	e \ref{cap:conclusao}).
	
	O DWPSO, é uma variante do GBPSO, variando no decorrer da aplicação o peso da inércia.
	A atualização da velocidade das part\'iculas \'e feita segundo a Eq. \ref{eq:DWPSO}, 
	considerando o $w_0$, o peso determinado para a primeira iteração e o $w_f$, o 
	peso determinado para a \'ultima iteração. 

	\begin{equation}
		V_{id} = w_i * V_{id} + C_1 * rand() * (P_{id} - X_{id}) + 
		C_2 * rand() * (P_{gd} - X_{id})
		\label{eq:DWPSO}
	\end{equation}
	\begin{equation}
		w_i = w_0 - (w_0 - w_f)\frac{i}{N}
	\end{equation}
	onde $i$ é o número da iteração atual e $N$ o número total de iteração a serem realizadas. 
	
	Assim, o peso da inércia diminui com o passar do tempo, fazendo com que deixe que o algoritmo
	se comporte e forma que explore mais o espaço de busca no inicio da execução, adquirindo
	bastante conhecimento do espaço e que mais pro final das iterações ele passe a focar mais
	onde ele conheceu como melhor no início. 

\section{Time-Varying Acceleration Coefficients PSO}
      O TVACPSO, é um algorítmo que adapta três parâmetros no decorrer da aplicação.
      Assim como o \ref{sec:DWPSO}, ele adapta o peso da inércia, mas também, 
      ajusta dois parâmetros denomidados, aceleração cognitiva e aceleração social.
      Esses dois parâmetros são de extrema importancia para o PSO. São eles que definem
      o aprendizado da partícula e do enxame. A aceleração cognitiva, asim como o nome
      diz, cognição significa aprender, corresponde a quanto uma partícula pode 
      aprender, ou seja, o quanto de informação que ela obteém ela vai ser capaz de
      ``memorizar''. Já a aceleração social, está ligada a quanto o enxame como um todo
      é capaz de aprender. 
      
      O balanço desses dois parâmetros é muito importante pois é ele quem diz como
      o algotítmo vai se comportar, levando mais em conta o que a partícula aprendeu
      e fazer com que cada partícula utilize o seu aprendizado e um pouco do aprendizado 
      do bando, ou se ele vai priorizar o que o bando aprendeu e dar menos importância
      para o individual.
      
      Em geral, esses dois parâmetros são definidos com o valor igual a 2, porém, sua eficiência
      nem sempre é satisfatória. A definição de todos esses parâmetros é muito trabalhosa, 
      e como a má definição leva a um fracasso a aplicação,
      o \textit{Time-Varying Acceleration Coefficients PSO} 
      (TVACPSO) (Ratnaweera et al., 2004), ajusta esses valores no decorrer da aplicação, deixando o 
      algoritmo mais flex\'ivel.
%		
      \begin{equation}
	      V_{id} = w_i * V_{id} + C_{1(i)} * rand() * (P_{id} - X_{id}) + 
	      C_{2(i)} * rand() * (P_{gd} - X_{id})
      \end{equation}
%		
      \begin{equation}
	      \begin{array}{lr}
	      C_{1(i)} = C_{1(0)} - (C_{1(0)} - C_{1(f)})\frac{i}{N}, & 
	      C_{2(i)} = C_{2(0)} - (C_{2(0)} - C_{2(f)})\frac{i}{N} \\
	      \end{array}
      \end{equation}

\section{Fully Informed PSO}
	O modelo denominado \textit{Fully Informed PSO} (FIPSO) (Mendes et al., 2004) %\cite{mendes2004fully} 
	sugere que a partícula deve ser influenciada por todas as outras part\'iculas ao seu redor.
	No entanto, a quantidade de vizinhos afetará positivamente, ou negativamente devido
	à grande variedade de influ\^encias sofrida pela partícula. O que difere muito de outros modelos 
	é que neles a partícula sofre influ\^encia somente da melhor part\'icula, descartando as demais.
	Sua adaptação é feita da seguinte maneira:
	
	\begin{equation}
		\begin{array}{lr}
		V_{id} = \chi( V_{id} + \varphi(P_m - X_{id})), \quad\quad& 
		P_m = \frac{C_1 * P_i + C_2 * P_g}{C_1 + C_2} \\
		\end{array}
	\end{equation}
	
	onde $\varphi = C_1 + C_2$, $\chi$ é o fator de constrição.
	
\chapter{Algoritmos e Modelo de Ilhas Paralelizadas}
	Aqui será descrito de maneira reduzida sobre dois algorítmos (DE e AG) utilizados
	no teste híbrido, além disso, ainda será esplicado o paralelismo utilizado
	e modelo de ilhas com suas políticas de migração de indivíduos para cada ilha.

	\section{Modelo de Ilhas Paralelizadas}
		Os algortmos evolucionistas, possuem uma carcterística muito interessante
		de serem natualmente paralelizaveis pelo fato das avaliações ...
		

\chapter{Experimentos Computacionais\label{cap:ExpComp}}
	Nesta parte do trabalho, foram implementados os 5 algorítmos do PSO descritos anteriormente
	para então rodar sobre um benchmarck elaborado para testes de desempenho
	de algorítmos evolucionista (CEC 2005) considerando os problemas de otimização sem
	restrição (caso de 10 dimensões).
	
	\section{Experimentos}
	Primeiramente, foram avaliados cada algoritmo individualmente sobre 6 conjuntos de
	parâmetros diferentes (tabelas \ref{tab:conf1} e \ref{tab:conf2}). Dentre esses 
	foram selecionadas as melhores configurações, ou seja, as definições de parâmetro 
	que chegou mais próximo do ótimo conhecido nos 25 problemas. 

	\begin{table}[h]
	 	\caption{Conjuntos de configurações para os algoritmo do PSO sem adaptação.}
	 	\centering
	 	\subfloat[Configurações para o PSO]{
			{\large
				\begin{tabular}{cccc}
					\hline\multicolumn{4}{c}{PSO}\\ \hline
						  & C1      & C2          & W       \\
					Conj1     & $2.0000$  & $2.0000$  & $0.9000$\\
					Conj2     & $-0.2746$ & $4.8976$  & $0.4000$\\
					Conj3     & $-0.2746$ & $4.8976$  & $0.9000$\\
					Conj4     & $-0.2699$ & $3.3950$  & $0.9000$\\
					Conj5     & $-0.6485$ & $2.6475$  & $0.9000$\\
					Conj6     & $-0.1565$ & $3.8876$  & $0.9000$\\\hline
				\end{tabular}
			}
		 }
	 	\subfloat[Configurações para o GBPSO]{
			{\large
				\begin{tabular}{cccc}
					\hline\multicolumn{4}{c}{GBPSO}             \\ \hline
						  & C1       & C2        & W        \\
					Conj1     & $2.0000$  & $2.0000$  & $ 0.9000$         \\
					Conj2     & $-0.2746$ & $4.8976$  & $-0.3488$         \\
					Conj3     & $-0.2746$ & $4.8976$  & $ 0.9000$         \\
					Conj4     & $-0.2699$ & $3.3950$  & $-0.4438$         \\
					Conj5     & $-0.6485$ & $2.6475$  & $-0.6031$         \\
					Conj6     & $-0.1565$ & $3.8876$  & $-0.2256$         \\\hline
				\end{tabular}
			}
	 	}
		\label{tab:conf1}
	\end{table}
	\begin{table}[h]
	 	\caption{Conjuntos de configurações para os algoritmo do PSO adaptativos.}
	 	\centering
	 	\subfloat[Configurações para o DWPSO]{
			{\large
				\begin{tabular}{ccccc}
					\hline\multicolumn{5}{c}{DWPSO}\\ \hline
					  & C1         & C2        & Wi         & Wf\\
				Conj1     & $ 2.0000$  & $2.0000$  & $ 0.9000$  & $ 0.4000$\\
				Conj2     & $-0.2746$  & $4.8976$  & $ 0.2000$  & $-0.3488$\\
				Conj3     & $-0.2746$  & $4.8976$  & $-0.3488$  & $ 0.2000$\\
				Conj4     & $-0.2699$  & $3.3950$  & $ 0.0001$  & $-0.4438$\\
				Conj5     & $-0.6485$  & $2.6475$  & $-0.1031$  & $-0.6031$\\
				Conj6     & $-0.1565$  & $3.8876$  & $ 0.3000$  & $-0.2256$\\\hline
				\end{tabular}
			}
		 }\\
	 	\subfloat[Configurações para o TVACPSO]{
			{\large
				\begin{tabular}{ccccccc}
				 \hline\multicolumn{7}{c}{TVACPSO}\\ \hline
					  &    C1i       &  C1f       & C2i       & C2f       & Wi         & Wf \\
				Conj1     &   $ 2.0000$  & $ 0.1000$  & $0.1000$  & $2.0000$  & $ 0.9000$  & $ 0.4000$         \\
				Conj2     &   $-0.2746$  & $-2.2746$  & $2.8976$  & $4.8976$  & $ 0.2000$  & $-0.3488$         \\
				Conj3     &   $-0.2746$  & $-2.2746$  & $2.8976$  & $4.8976$  & $ 0.4000$  & $ 0.9000$         \\
				Conj4     &   $-0.2699$  & $-2.2699$  & $1.3950$  & $3.3950$  & $ 0.1000$  & $-0.4438$         \\
				Conj5     &   $-0.6485$  & $-2.6485$  & $0.6475$  & $2.6475$  & $-0.1031$  & $-0.6031$         \\
				Conj6     &   $-0.0787$  & $-2.0787$  & $1.7637$  & $3.7637$  & $ 0.4787$  & $-0.0787$         \\\hline
				\end{tabular}
			}
	 	}
		\label{tab:conf2}
	\end{table}

	
	Logo após, com os melhores de cada, foram analizados comparando não mais o algoritmo
	individualmente, e sim, cada algorítmo com os demais para então saber qual deles 
	se sobre saiu nos mesmos 25 problemas.

\chapter{Conclusões e trabalhos futuros\label{cap:conclusao}}

\section{Conclusões}


\bibliographystyle{abnt-alf}
\bibliography{monografia}


\anexo

\chapter{Ferramentas utilizadas}

Foi feita uma análise de algumas ferramentas...
\end{document}
